# -*- coding: utf-8 -*-
"""Twitter-Sentiment Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HLcLbbd5Z2VR1q6eu6qR8RbWbt3X-qiK
"""

# Imports
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
import pandas as pd

"""Downloads of dataset"""

pip install gdown

!gdown 1o5MUso55NfTymcO_KsWAaLLd7py4yU6Q

!unzip /content/TwitterDataset.zip

"""Preprocessing of the Training and the Test Set"""

columns = ["Id","Entity","Target","Text"]
data_train = pd.read_csv("/content/HW2/twitter_training.csv",
                   names=columns,header=None)

columns = ["Id","Entity","Target","Text"]
data_test = pd.read_csv("/content/HW2/twitter_test.csv",
                   names=columns,header=None)

data_train.head(50)

#We take only the column "Text" and "Target"
df_train = data_train[["Text","Target"]]

df_train.head()

df_train.shape

df_test = data_test[["Text","Target"]]

df_test.head()

df_test.shape

df_toappend = df_train.sample(n = 6000, random_state=1)
df_toappend

df_toappend.index

for r in df_train.index:
    for i in df_toappend.index:
        if r==i:
            df_train = df_train.drop(index=[r])

df_train

df_test = df_test.append(df_toappend)
df_test

len(df_test)

for df in ["df_train", "df_test"]:
  if df == "df_train":
    df_train  = df_train.convert_dtypes()
    df_train  = df_train.dropna()
    print("The type of df_train: \n ", df_train.dtypes)
  else:
    df_test  = df_test.convert_dtypes()
    df_test  = df_test.dropna()
    print("\nThe type of df_test: \n ", df_test.dtypes)

"""Processing the tweets"""

# Remove punctuaction
from string import punctuation
punctuation

pip install clean-text

from cleantext import clean

for df in ["df_train", "df_test"]:
  if df == "df_train":
    tweets_train = df_train["Text"]
  else:
    tweets_test = df_test["Text"]

for df in ["df_train", "df_test"]:
  if df == "df_train":
    tweets_train = df_train["Text"]
  else:
    tweets_test = df_test["Text"]

for df in ["tweets_train", "tweets_test"]:
  if df == "tweets_train":
    list_characters = [[c for c in t if c not in punctuation] for t in tweets_train]
    tweets_train = ["".join(t).lower() for t in list_characters]
    tweets_train = [clean(t, no_emoji=True) for t in tweets_train]
    print("The tweets for the train are: ", tweets_train[:50])
    print("The lenght of tweets_train is: ",len(tweets_train))
  else:
    tweets_test = df_test["Text"]
    list_characters = [[c for c in t if c not in punctuation] for t in tweets_test]
    tweets_test = ["".join(t).lower() for t in list_characters]
    tweets_test = [clean(t, no_emoji=True) for t in tweets_test]
    print("\nThe tweets for the test are: ", tweets_test[:50])
    print("The lenght of tweets_test is: ",len(tweets_test))

for df in ["tweets_train", "tweets_test"]:
  if df == "tweets_train":
    tweets_train = [[w for w in t.replace("\n", " ").split(" ") if len(w)>0] for t in tweets_train]
    print("Tweet_train: ", tweets_train[0])
  else:
    tweets_test = [[w for w in r.replace("\n", " ").split(" ") if len(w)>0] for r in tweets_test]
    print("Tweet_test: ", tweets_test[0])

# compute the length of each tweets in tweets_train
tweets_lens_train = [len(r) for r in tweets_train]
# print the lenght of the first x tweets
tweets_lens_train[:10]

# print the average tweets lenght
sum(tweets_lens_train) / len(tweets_lens_train)

#Creation of a unique list of words
final_tweets = tweets_train + tweets_test

len(final_tweets)

final_tweets[50]

# build vocabulary
words = list(set([w for r in final_tweets for w in r]))
vocab = {words[i]: i+1 for i in range(len(words))} # we reserve i=0 for pad sequences

len(vocab)

# Convert tweets to word indexes
for df in ["tweets_train", "tweets_test"]:
  if df == "tweets_train":
    tweets_train = [[vocab[w] for w in r] for r in tweets_train]
    print(tweets_train[0])
  else:
    tweets_test = [[vocab[w] for w in r] for r in tweets_test]
    print(tweets_test[0])

#For example, we can print the word corrispondent to the first element of tweets_train
words[tweets_train[0][0]-1]

"""The reviews have different lengths, while, we want that all reviews have the same length"""

seq_len = 15    # number of words for each sentence

for df in ["tweets_train", "tweets_test"]:
  if df == "tweets_train":
    # Clip tweets to max seq_len words
    tweets_train = [r[:seq_len] for r in tweets_train]
    
    # Print average tweets length now
    tweets_lens_train = [len(r) for r in tweets_train]
    print("Tweets of the train length before padding: ", sum(tweets_lens_train)/len(tweets_lens_train))
    
    # Pad tweets shorter than seq_len
    # TODO test padding at the end
    tweets_train = [[0]*(seq_len - len(r)) + r for r in tweets_train]
    
    # Print average tweet length now
    tweets_lens_train = [len(r) for r in tweets_train]
    print("Tweets of the train length after padding: ", sum(tweets_lens_train)/len(tweets_lens_train))
  else:
    tweets_test = [r[:seq_len] for r in tweets_test]
    
    # Print average tweet length now
    tweets_lens_test = [len(r) for r in tweets_test]
    print("Tweets of the test length before padding: ", sum(tweets_lens_test)/len(tweets_lens_test))
    
    # Pad tweets shorter than seq_len
    # TODO test padding at the end
    tweets_test = [[0]*(seq_len - len(r)) + r for r in tweets_test]
    
    # Print average tweet length now
    tweets_lens_test = [len(r) for r in tweets_test]
    print("Tweets of the test length after padding: ",sum(tweets_lens_test)/len(tweets_lens_test))

"""Now, we can convert tweets_train and tweets_test in a Tensor:"""

for df in ["tweets_train", "tweets_test"]:
  if df == "tweets_train":
    # Convert tweets to tensor
    train = torch.LongTensor(tweets_train)
    print("The size of the train is: ", train.size())
  else:
    test_data = torch.LongTensor(tweets_test)
    print("The size of the train is: ", test_data.size())

"""Processing the labels"""

label_train = df_train["Target"]

len(label_train)

label_test = df_test["Target"]

len(label_test)

import seaborn as sns
sns.countplot(x="Target",data=df_train).set(title='Labels for Train')

label_test = df_test["Target"]
sns.countplot(x="Target",data=df_test).set(title='Labels for Test')

for labels in ["label_train", "label_test"]:
  if labels == "label_train":
    sentiments_train = []
    for i in label_train:
      if i == "Positive":
        sentiments_train.append(2)
      elif (i == "Irrelevant") or (i == "Neutral"):
        sentiments_train.append(1)
      else:
        sentiments_train.append(0)
    df_train["Sentiments"] = sentiments_train
  else:
    sentiments_test = []
    for i in label_test:
      if i == "Positive":
        sentiments_test.append(2)
      elif (i == "Irrelevant") or (i == "Neutral"):
        sentiments_test.append(1)
      else:
        sentiments_test.append(0)
    df_test["Sentiments"] = sentiments_test

df_train.head()

sns.countplot(x="Sentiments",data=df_train).set(title='Numeric labels for Train')

sns.countplot(x="Sentiments",data=df_test).set(title='Numeric labels for Test')

# Convert sentiments to tensor for train
labels_train = torch.LongTensor(sentiments_train)
print("The size of the labels_train is: ", labels_train.size())

# Convert sentiments to tensor for train
test_labels = torch.LongTensor(sentiments_test)
print("The size of the labels_test is: ", test_labels.size())

"""Defining the Datasets"""

num_train = len(tweets_train)
num_test = len(tweets_test)
print(f"Num. training samples: {num_train}")
print(f"Num. test samples:     {num_test}")

#Validation set
val_frac= 0.1

# shuffle dataset
num_data = train.size(0)
shuffle_idx = torch.randperm(num_data)
train = train[shuffle_idx,:]
labels_train = labels_train[shuffle_idx]

# split training, validation and test
num_val = int(num_data*val_frac)
num_train = num_data - num_val
train_data = train[:num_train,:]
train_labels = labels_train[:num_train]
val_data = train[num_train:num_train+num_val,:]
val_labels = labels_train[num_train:num_train+num_val]

print(train_data.size())
print(val_data.size())
print(test_data.size())

train_data = torch.LongTensor(train_data)
val_data = torch.LongTensor(val_data)
test_data = torch.LongTensor(test_data)

train_labels = torch.LongTensor(train_labels)
val_labels = torch.LongTensor(val_labels)
test_labels = torch.LongTensor(test_labels)

print(train_data.size())
print(val_data.size())
print(test_data.size())

print(train_labels.size())
print(val_labels.size())
print(test_labels.size())

# create datasets
train_dataset = TensorDataset(train_data, train_labels)
val_dataset = TensorDataset(val_data, val_labels)
test_dataset = TensorDataset(test_data, test_labels)

#Define the batch size
batch_size = 256

# print the first elem of train dataset and its label
print(train_dataset[0])

# Create loaders
loaders = {"train": DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  drop_last=True),
           "val":   DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, drop_last=False),
           "test":  DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, drop_last=False)}

"""## Defining our RNN"""

# Define model
#class Model(nn.Module):
#    
#    def __init__(self, num_embed, embed_size, rnn_size):
#        # params: 
#        # num_embed: the number of the input vocabulary
#        # embed_size: the size of the feature embedding
#       # rnn_size: the number of neurons in the recurrent layer
#
#        # Call parent constructor
#        super().__init__()
#        # Store values
#        self.rnn_size = rnn_size
 #       # Define modules
#        self.embedding = nn.Embedding(len(vocab_train)+1, embed_size)
#        self.rnn = nn.RNNCell(embed_size, rnn_size) #RNNCell represents only a single time step
#        self.output = nn.Linear(rnn_size, 3)
#        self.softmax = nn.Softmax(dim=1)
#        
#    def forward(self, x):
#        # Embed data
#        x = self.embedding(x)
#        # Initialize state
#        h = torch.zeros(x.shape[0], self.rnn_size).to(x.device.type) # the state of the cell
#        
#        # Input is: B x T x F
#       # Process each time step
#        for t in range(x.shape[1]):
#            # Input at time t
#            x_t = x[:,t,:]
#            # Forward RNN and get new state
#            h = self.rnn(x_t, h)
#        # Classify final state
#        x = self.output(h)
#        x = self.softmax(x)
#        return x

# Define model
class Model(nn.Module):
    
    def __init__(self, num_embed, embed_size, rnn_size):
        # params: 
        # num_embed: the number of the input vocabulary
        # embed_size: the size of the feature embedding
        # rnn_size: the number of neurons in the recurrent layer

        # Call parent constructor
        super().__init__()
        # Store values
        self.rnn_size = rnn_size
        # Define modules
        self.embedding = nn.Embedding(num_embed, embed_size)
        self.LSTM = nn.LSTMCell(embed_size, rnn_size) #LSTMCell receaves only a single time step
        
        self.output = nn.Linear(rnn_size, 3)
        self.softmax = nn.Softmax(dim=1)
        
    def forward(self, x):
        # Embed data
        x = self.embedding(x)
        # Initialize state
        c = torch.zeros(x.shape[0], self.rnn_size).to(x.device.type) # the state of the cell
        h = torch.zeros(x.shape[0], self.rnn_size).to(x.device.type) # the output of the cell
        
        # Input is: B x T x F
        # Process each time step
        for t in range(x.shape[1]):
            # Input at time t
            x_t = x[:,t,:]
            # Forward RNN and get new state
            h,c = self.LSTM(x_t, (h, c))  # the LSTMCell wants the input(x_t) and the previous state (h and c)
        
        # at the end of the loop, h is the final output of the LSTMCell after processing all the sequence

        # Classify final state        
        out= self.output(h)
        x = self.softmax(out)
        return x

# Setup device
dev = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#dev = "cpu"

# Model parameters
embed_size = 1024
rnn_size = 512

# Create model
model = Model((len(vocab)+1), embed_size, rnn_size).to(dev)

# Test model output
model.eval()
test_input = train_dataset[0][0].unsqueeze(0).to(dev)
print("Test input size: ", test_input.size())
print("Model output size:", model(test_input).size())

# Create optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)

# Define a loss 
criterion = nn.CrossEntropyLoss()

# Start training
from tqdm import tqdm
for epoch in range(100):
    # Initialize accumulators for computing average loss/accuracy
    epoch_loss_sum = {'train': 0, 'val': 0, 'test': 0}
    epoch_loss_cnt = {'train': 0, 'val': 0, 'test': 0}
    epoch_accuracy_sum = {'train': 0, 'val': 0, 'test': 0}
    epoch_accuracy_cnt = {'train': 0, 'val': 0, 'test': 0}
    # Process each split
    for split in ["train", "val", "test"]:
        # Set network mode
        if split == "train":
            model.train()
            torch.set_grad_enabled(True)
        else:
            model.eval()
            torch.set_grad_enabled(False)
        # Process all data in split
        for input, target in tqdm(loaders[split]):
            # Move to device
            input = input.to(dev)
            target = target.to(dev)
            # Reset gradients
            optimizer.zero_grad()
            # Forward
            output = model(input)
            loss = criterion(output, target)
            # Update loss sum
            epoch_loss_sum[split] += loss.item()
            epoch_loss_cnt[split] += 1
            # Compute accuracy
            _,pred = output.max(1)
            correct = pred.eq(target).sum().item()
            accuracy = correct/input.size(0)
            # Update accuracy sum
            epoch_accuracy_sum[split] += accuracy
            epoch_accuracy_cnt[split] += 1
            # Backward and optimize
            if split == "train":
                loss.backward()
                optimizer.step()
    # Compute average epoch loss/accuracy
    avg_train_loss = epoch_loss_sum["train"]/epoch_loss_cnt["train"]
    avg_train_accuracy = epoch_accuracy_sum["train"]/epoch_accuracy_cnt["train"]
    avg_val_loss = epoch_loss_sum["val"]/epoch_loss_cnt["val"]
    avg_val_accuracy = epoch_accuracy_sum["val"]/epoch_accuracy_cnt["val"]
    avg_test_loss = epoch_loss_sum["test"]/epoch_loss_cnt["test"]
    avg_test_accuracy = epoch_accuracy_sum["test"]/epoch_accuracy_cnt["test"]
    print(f"Epoch: {epoch+1}, TrL={avg_train_loss:.4f}, TrA={avg_train_accuracy:.4f},",
                            f"VL={avg_val_loss:.4f}, VA={avg_val_accuracy:.4f}, ",
                            f"TeL={avg_test_loss:.4f}, TeA={avg_test_accuracy:.4f}")

#CONFUSION MATRIX
#function that perform inference
def predict(net, loader,dev):
  net.to(dev)
  predictions_list = []
  with torch.no_grad():
    for (input, labels) in loader:
      # Move to CUDA
      input = input.to(dev)
      pred = net(input)
      _,pred_labels = pred.max(1)
      pred_labels.tolist()
      for lab in pred_labels:
        lab = lab.item()
        predictions_list.append(str(lab))
  return predictions_list

#inference on test set
predicted_labels = predict(model, loaders["test"], dev=dev)

#extracting ground truth labels
true_labels = []
for _ ,label in loaders["test"]:
  for lab in label:
    lab = lab.item()
    true_labels.append(str(lab))

classes = [0, 1, 2]

labels = [str(x) for x in classes]
print (labels)

#confusion matrix
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(y_true=true_labels, y_pred=predicted_labels, labels=labels)
cm

import seaborn as sns
import numpy as np
from matplotlib import pyplot as plt

cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
fig, ax = plt.subplots(figsize=(10,10))
sns.heatmap(cmn, annot=True, fmt='.2f', xticklabels=classes, yticklabels=classes)
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show(block=False)





















